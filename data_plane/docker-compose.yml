# =============================================================================
# Data Plane Infrastructure Services
# =============================================================================
#
# Modes:
#   standalone: No control plane, use static config only
#   connected:  Use control plane API, fall back to static config
#
# Usage:
#   Standard mode (gVisor - RECOMMENDED for production):
#     docker-compose --profile standard up -d
#     Requires gVisor: https://gvisor.dev/docs/user_guide/install/
#
#   Development mode (no gVisor - for local dev or if gVisor unavailable):
#     docker-compose --profile dev up -d
#
#   With local admin UI + email proxy:
#     docker-compose --profile standard --profile admin up -d
#     Open http://localhost:8081
#     Note: In connected mode (DATAPLANE_MODE=connected), admin UI is read-only
#
#   With auditing (log forwarding):
#     docker-compose --profile standard --profile auditing up -d
#
#   With direct SSH access (port mapping):
#     SSH_AUTHORIZED_KEYS="ssh-ed25519 AAAA..." docker-compose --profile dev up -d
#     ssh -p 2222 agent@localhost
#
#   With SSH tunnel (FRP/STCP for remote access):
#     docker-compose --profile standard --profile ssh up -d
#
# Network Architecture:
#   agent-net (isolated, internal)
#     └── agent(s): can ONLY reach http-proxy + dns-filter (dynamic IPs)
#     Scale with: docker compose --scale agent-dev=N
#
#   infra-net (can reach control plane)
#     └── http-proxy, dns-filter (bridged to agent-net)
#     └── log-shipper (log forwarding) - optional
#
# Security Controls:
#   - Agent has no direct internet/CP access (IP bypass prevention)
#   - Credential injection handled by HTTP proxy Lua filter
#   - Only HTTP proxy and log shipper can reach control plane
#
# =============================================================================

services:
  # ===========================================================================
  # Agent Container (DEVELOPMENT - runc runtime, no gVisor)
  # Use when gVisor is not installed or for local development
  # For production, use --profile standard (gVisor) instead
  # ===========================================================================
  agent-dev:
    build:
      context: .
      dockerfile: agent.Dockerfile
      args:
        - VARIANT=${AGENT_VARIANT:-lean}
    image: agent:${AGENT_VARIANT:-lean}
    # No container_name — allows `docker compose --scale agent-dev=N`
    labels:
      - "cagent.role=agent"
      - "cagent.log-collect=true"
    profiles:
      - dev

    # Standard container runtime (no gVisor)
    runtime: runc

    # Security hardening
    cap_drop:
      - NET_RAW
    security_opt:
      - no-new-privileges:true
      # Block raw sockets to prevent proxy bypass via packet crafting
      - seccomp:./configs/seccomp/agent-profile.json

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 512M

    # DNS forced through dns-filter
    dns:
      - 10.200.1.5

    # Environment
    environment:
      - HTTP_PROXY=http://10.200.1.10:8443
      - HTTPS_PROXY=http://10.200.1.10:8443
      - http_proxy=http://10.200.1.10:8443
      - https_proxy=http://10.200.1.10:8443
      - NO_PROXY=localhost,127.0.0.1
      - no_proxy=localhost,127.0.0.1
      - USER_NAME=agent
      # SSH public keys (paste your public key here or mount file)
      - SSH_AUTHORIZED_KEYS=${SSH_AUTHORIZED_KEYS:-}
      # Tmux session settings
      - TMUX_AUTO_ATTACH=${TMUX_AUTO_ATTACH:-1}  # Set to 0 to disable auto-attach
      - TMUX_SESSION=${TMUX_SESSION:-main}       # Default session name

    # SSH access (set SSH_AUTHORIZED_KEYS to enable)
    ports:
      - "${SSH_PORT:-2222}:22"

    # Volumes
    volumes:
      - agent-logs:/var/log/agent
      - agent-workspace:/workspace
      # Optional: mount SSH keys file
      # - ./ssh-keys:/ssh-keys:ro

    # ONLY on agent-net - cannot reach infra-net or external directly
    # IP auto-assigned from 10.200.1.0/24 (static IPs only on infra services)
    networks:
      agent-net: {}

    # Capture output to Docker logs (read by log shipper)
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "5"

    healthcheck:
      test: ["CMD-SHELL", "kill -0 1"]
      interval: 30s
      timeout: 5s
      retries: 3

    depends_on:
      - http-proxy
      - dns-filter

    restart: unless-stopped

  # ===========================================================================
  # Agent Container (STANDARD - gVisor isolation, recommended for production)
  # Requires gVisor: https://gvisor.dev/docs/user_guide/install/
  # Use --profile dev if gVisor is not installed
  # ===========================================================================
  agent:
    build:
      context: .
      dockerfile: agent.Dockerfile
      args:
        - VARIANT=${AGENT_VARIANT:-lean}
    image: agent:${AGENT_VARIANT:-lean}
    # No container_name — allows `docker compose --scale agent=N`
    labels:
      - "cagent.role=agent"
      - "cagent.log-collect=true"
    profiles:
      - standard

    # gVisor runtime - syscalls intercepted in user-space
    runtime: runsc

    # Strict security options
    security_opt:
      - no-new-privileges:true
      - seccomp:./configs/seccomp/agent-profile.json  # defense-in-depth: host seccomp + gVisor

    # Stricter resource limits for secure mode
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 256M

    # DNS forced through dns-filter
    dns:
      - 10.200.1.5

    # Environment
    environment:
      - HTTP_PROXY=http://10.200.1.10:8443
      - HTTPS_PROXY=http://10.200.1.10:8443
      - http_proxy=http://10.200.1.10:8443
      - https_proxy=http://10.200.1.10:8443
      - NO_PROXY=localhost,127.0.0.1
      - no_proxy=localhost,127.0.0.1
      - USER_NAME=agent
      - SSH_AUTHORIZED_KEYS=${SSH_AUTHORIZED_KEYS:-}
      - TMUX_AUTO_ATTACH=${TMUX_AUTO_ATTACH:-1}
      - TMUX_SESSION=${TMUX_SESSION:-main}

    # SSH access (set SSH_AUTHORIZED_KEYS to enable)
    ports:
      - "${SSH_PORT:-2222}:22"

    # Volumes (more restricted in secure mode)
    volumes:
      - agent-logs:/var/log/agent
      - agent-workspace:/workspace:rw

    # ONLY on agent-net
    # IP auto-assigned from 10.200.1.0/24 (static IPs only on infra services)
    networks:
      agent-net: {}

    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

    healthcheck:
      test: ["CMD-SHELL", "kill -0 1"]
      interval: 30s
      timeout: 5s
      retries: 3

    depends_on:
      - http-proxy
      - dns-filter

    restart: unless-stopped

  # ===========================================================================
  # DNS Filter - CoreDNS with domain allowlist
  # Bridged to both networks so agent can resolve DNS
  # Corefile generated by agent-manager from CP allowlist + static config
  # ===========================================================================
  dns-filter:
    image: coredns/coredns:latest
    container_name: dns-filter
    labels:
      - "cagent.log-collect=true"
    # Scaling: to run >1 replica, remove container_name and static IPs, add a load balancer
    # deploy:
    #   replicas: ${DNS_FILTER_REPLICAS:-1}

    volumes:
      # Corefile from host (static default, overwritten by agent-manager when it syncs)
      - ./configs/coredns:/etc/coredns

    command: ["-conf", "/etc/coredns/Corefile"]

    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3

    # On BOTH networks - agent-net for agent access, infra-net for upstream
    networks:
      agent-net:
        ipv4_address: 10.200.1.5
      infra-net:
        ipv4_address: 10.200.2.5

    restart: unless-stopped

  # ===========================================================================
  # HTTP Proxy - Egress gateway with logging and credential injection
  # Config generated by agent-manager from cagent.yaml
  # ===========================================================================
  http-proxy:
    image: envoyproxy/envoy:v1.28-latest
    container_name: http-proxy
    labels:
      - "cagent.log-collect=true"
    # Scaling: to run >1 replica, remove container_name and static IPs, add a load balancer
    # deploy:
    #   replicas: ${HTTP_PROXY_REPLICAS:-1}

    volumes:
      # Generated config from agent-manager (falls back to static if not present)
      - proxy-config:/etc/envoy
      # Static config as fallback (copied by entrypoint if no generated config)
      - ./configs/envoy/envoy-enhanced.yaml:/etc/envoy/envoy.default.yaml:ro
      # Static Lua filter as fallback (copied by entrypoint if no generated filter)
      - ./configs/envoy/filter.lua:/etc/envoy/filter.default.lua:ro
      # Entrypoint script
      - ./configs/envoy/entrypoint.sh:/entrypoint.sh:ro

    entrypoint: ["/bin/sh", "/entrypoint.sh"]

    # No env vars needed: Lua filter talks to agent-manager (local, no auth)

    # JSON logs to stdout (captured by Docker, read by log shipper)
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

    # On BOTH networks - agent-net for agent access, infra-net for egress + CP access
    networks:
      agent-net:
        ipv4_address: 10.200.1.10
      infra-net:
        ipv4_address: 10.200.2.10

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 128M

    restart: unless-stopped

    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/9901'"]
      interval: 10s
      timeout: 5s
      retries: 3

  # ===========================================================================
  # Log Shipper - Log Collection & Forwarding
  # Collects Docker logs, gVisor audit logs, agent app logs
  # Optional: Only runs with --profile auditing
  # ===========================================================================
  log-shipper:
    image: timberio/vector:0.36.0-alpine
    container_name: log-shipper
    profiles:
      - auditing

    volumes:
      - ./configs/vector/vector.yaml:/etc/vector/vector.yaml:ro
      - agent-logs:/var/log/agent:ro
      - log-shipper-data:/var/lib/vector
      - log-shipper-backup:/var/log/vector
      # Docker socket for container log collection
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # gVisor logs (requires runsc --debug-log=/var/log/runsc/)
      - /var/log/runsc:/var/log/runsc:ro

    environment:
      # Control plane API (CP-mediated log ingestion)
      - CONTROL_PLANE_URL=${CONTROL_PLANE_URL}
      - CONTROL_PLANE_TOKEN=${CONTROL_PLANE_TOKEN}
      - ENVIRONMENT=${ENVIRONMENT:-development}

    command: ["--config", "/etc/vector/vector.yaml"]

    # Only on infra-net - can reach control plane
    networks:
      - infra-net

    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 64M

    restart: unless-stopped

    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8686/health || kill -0 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # Agent Manager - Unified data plane service
  # Handles config generation, container management, CP sync, and admin UI
  # In admin profile: includes local admin UI (frontend + full API)
  # In managed profile: API-only (no frontend)
  # ===========================================================================
  agent-manager:
    build:
      context: ..
      dockerfile: data_plane/services/agent_manager/Dockerfile
    container_name: agent-manager
    labels:
      - "cagent.log-collect=true"
    profiles:
      - managed
      - admin

    environment:
      # Operation mode: "standalone" or "connected"
      - DATAPLANE_MODE=${DATAPLANE_MODE:-standalone}
      # Control plane connection (for connected mode)
      - CONTROL_PLANE_URL=${CONTROL_PLANE_URL}
      - CONTROL_PLANE_TOKEN=${CONTROL_PLANE_TOKEN}
      - HEARTBEAT_INTERVAL=${HEARTBEAT_INTERVAL:-30}
      # Config paths
      - CAGENT_CONFIG_PATH=/etc/cagent/cagent.yaml
      - COREDNS_COREFILE_PATH=/etc/coredns/Corefile
      - ENVOY_CONFIG_PATH=/etc/envoy/envoy.yaml
      - CONFIG_SYNC_INTERVAL=${CONFIG_SYNC_INTERVAL:-300}
      - SECCOMP_PROFILES_DIR=/etc/seccomp/profiles
      # Local admin settings
      - DATA_PLANE_DIR=/app/data_plane
      - BETA_FEATURES=${BETA_FEATURES:-}

    volumes:
      # Docker socket for container management
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Main config file (read-write for admin UI config editing)
      - ./configs/cagent.yaml:/etc/cagent/cagent.yaml
      # CoreDNS config directory (agent-manager overwrites Corefile here)
      - ./configs/coredns:/etc/coredns
      # HTTP proxy config volume (agent-manager writes generated config here)
      - proxy-config:/etc/envoy
      # Seccomp profiles for container recreation
      - ./configs/seccomp/profiles:/etc/seccomp/profiles:ro
      # Data plane directory (for .env and docker-compose access)
      - .:/app/data_plane

    ports:
      - "${LOCAL_ADMIN_PORT:-8081}:8080"

    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3

    networks:
      - infra-net

    restart: unless-stopped

  # ===========================================================================
  # Email Proxy - Controlled email access (IMAP/SMTP) for agents
  # Supports Gmail (OAuth2), Outlook (OAuth2), and generic IMAP/SMTP
  # Optional: Only runs with --profile email
  # ===========================================================================
  email-proxy:
    build:
      context: ./services/email_proxy
    container_name: email-proxy
    labels:
      - "cagent.log-collect=true"
    profiles:
      - email

    environment:
      - CAGENT_CONFIG_PATH=/etc/cagent/cagent.yaml
      # Gmail OAuth (set in .env or shell)
      - GMAIL_CLIENT_ID=${GMAIL_CLIENT_ID:-}
      - GMAIL_CLIENT_SECRET=${GMAIL_CLIENT_SECRET:-}
      - GMAIL_REFRESH_TOKEN=${GMAIL_REFRESH_TOKEN:-}
      # Outlook/M365 OAuth (set in .env or shell)
      - OUTLOOK_CLIENT_ID=${OUTLOOK_CLIENT_ID:-}
      - OUTLOOK_CLIENT_SECRET=${OUTLOOK_CLIENT_SECRET:-}
      - OUTLOOK_REFRESH_TOKEN=${OUTLOOK_REFRESH_TOKEN:-}
      # Generic mail password (set in .env or shell)
      - MAIL_PASSWORD=${MAIL_PASSWORD:-}

    volumes:
      - ./configs/cagent.yaml:/etc/cagent/cagent.yaml:ro

    # On infra-net only - agent accesses via Envoy on agent-net
    networks:
      infra-net:
        ipv4_address: 10.200.2.40

    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8025/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

    restart: unless-stopped

  # ===========================================================================
  # Tunnel Client - Reverse tunnel for SSH access via STCP
  # Connects outbound to control plane tunnel server
  # STCP mode: tunneling via FRP server with unique secret key per agent
  # Optional: Only runs with --profile ssh
  # ===========================================================================
  tunnel-client:
    image: snowdreamtech/frpc:latest
    container_name: tunnel-client
    profiles:
      - ssh

    entrypoint: ["/bin/sh", "/bootstrap/entrypoint.sh"]

    environment:
      # FRP server auth (must match frps.toml on control plane)
      - FRP_AUTH_TOKEN=${FRP_AUTH_TOKEN}
      # FRP server address (required - e.g. 192.168.1.50)
      - FRP_SERVER_ADDR=${FRP_SERVER_ADDR}
      - FRP_SERVER_PORT=${FRP_SERVER_PORT:-7000}

    volumes:
      - ./configs/frpc/entrypoint.sh:/bootstrap/entrypoint.sh:ro

    # On BOTH networks:
    # - agent-net: to reach agent SSH (uses Docker DNS: agent-dev or agent)
    # - infra-net: to reach tunnel server (outbound) and agent-manager API
    networks:
      agent-net:
        ipv4_address: 10.200.1.30
      infra-net:
        ipv4_address: 10.200.2.30

    healthcheck:
      test: ["CMD-SHELL", "kill -0 1"]
      interval: 30s
      timeout: 5s
      retries: 3

    depends_on:
      - agent
      - agent-manager

    restart: unless-stopped

# =============================================================================
# Networks
# =============================================================================
networks:
  # Agent network - INTERNAL, no external access
  agent-net:
    driver: bridge
    internal: true  # No external connectivity
    enable_ipv6: false  # Prevent IPv6 bypass of egress controls
    ipam:
      config:
        - subnet: 10.200.1.0/24

  # Infrastructure network - can reach control plane
  infra-net:
    driver: bridge
    enable_ipv6: false  # Prevent IPv6 bypass of egress controls
    ipam:
      config:
        - subnet: 10.200.2.0/24

# =============================================================================
# Volumes
# =============================================================================
volumes:
  agent-logs:
  agent-workspace:
  proxy-config:
  log-shipper-data:
  log-shipper-backup:
