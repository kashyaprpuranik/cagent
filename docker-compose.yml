# =============================================================================
# Data Plane Infrastructure Services
# =============================================================================
#
# Modes:
#   standalone: No control plane, use static config only
#   connected:  Use control plane API, fall back to static config
#
# Usage:
#   Standard mode (gVisor - RECOMMENDED for production):
#     docker-compose --profile standard up -d
#     Requires gVisor: https://gvisor.dev/docs/user_guide/install/
#
#   Development mode (no gVisor - for local dev or if gVisor unavailable):
#     docker-compose --profile dev up -d
#
#   With local admin UI + email proxy:
#     docker-compose --profile standard --profile admin up -d
#     Open http://localhost:8081
#     Note: In connected mode (DATAPLANE_MODE=connected), admin UI is read-only
#
#   With auditing (log forwarding):
#     docker-compose --profile standard --profile auditing up -d
#
#   With direct SSH access (port mapping):
#     SSH_AUTHORIZED_KEYS="ssh-ed25519 AAAA..." docker-compose --profile dev up -d
#     ssh -p 2222 cell@localhost
#
# Network Architecture:
#   cell-net (isolated, internal)
#     └── cell(s): can ONLY reach http-proxy + dns-filter (dynamic IPs)
#     Scale with: docker compose --scale cell-dev=N
#
#   infra-net (can reach control plane)
#     └── http-proxy, dns-filter (bridged to cell-net)
#     └── warden (config sync, admin UI, domain policy API)
#     └── log-shipper (log forwarding) - optional
#     └── email-proxy (IMAP/SMTP proxy) - optional, beta
#
# Security Controls:
#   - Cell has no direct internet/CP access (IP bypass prevention)
#   - Credential injection handled by HTTP proxy ext_authz filter (warden)
#   - Only HTTP proxy and log shipper can reach control plane
#
# =============================================================================

services:
  # ===========================================================================
  # Cell (DEVELOPMENT - runc runtime, no gVisor)
  # Use when gVisor is not installed or for local development
  # For production, use --profile standard (gVisor) instead
  # ===========================================================================
  cell-dev:
    build:
      context: .
      dockerfile: cell.Dockerfile
      args:
        - VARIANT=${CELL_VARIANT:-lean}
    image: cell:${CELL_VARIANT:-lean}
    # No container_name — allows `docker compose --scale cell-dev=N`
    labels:
      - "cagent.role=cell"
      - "cagent.log-collect=true"
    profiles:
      - dev

    # Standard container runtime (no gVisor)
    runtime: runc

    # Security hardening
    cap_drop:
      - NET_RAW
    security_opt:
      - no-new-privileges:true
      # Block raw sockets to prevent proxy bypass via packet crafting
      - seccomp:./configs/seccomp/cell-profile.json

    # Resource limits (configurable via cagent.yaml → warden → .env)
    deploy:
      resources:
        limits:
          cpus: '${CELL_CPU_LIMIT:-2}'
          memory: ${CELL_MEMORY_LIMIT:-4G}
          pids: ${CELL_PIDS_LIMIT:-256}
        reservations:
          cpus: '0.5'
          memory: 512M

    # DNS forced through dns-filter (dns-filter-2 failover via --profile dns-ha)
    dns:
      - 10.200.1.5
      - 10.200.1.6

    # Environment
    environment:
      - HTTP_PROXY=http://10.200.1.10:8443
      - HTTPS_PROXY=http://10.200.1.10:8443
      - http_proxy=http://10.200.1.10:8443
      - https_proxy=http://10.200.1.10:8443
      - NO_PROXY=localhost,127.0.0.1
      - no_proxy=localhost,127.0.0.1
      - USER_NAME=cell
      # SSH public keys (paste your public key here or mount file)
      - SSH_AUTHORIZED_KEYS=${SSH_AUTHORIZED_KEYS:-}
      # Tmux session settings
      - TMUX_AUTO_ATTACH=${TMUX_AUTO_ATTACH:-1}  # Set to 0 to disable auto-attach
      - TMUX_SESSION=${TMUX_SESSION:-main}       # Default session name

    # SSH access (set SSH_AUTHORIZED_KEYS to enable)
    ports:
      - "${SSH_PORT:-2222}:22"

    # Volumes
    volumes:
      - cell-logs:/var/log/cell
      - cell-workspace:/workspace
      # Optional: mount SSH keys file
      # - ./ssh-keys:/ssh-keys:ro

    # ONLY on cell-net - cannot reach infra-net or external directly
    # IP auto-assigned from 10.200.1.0/24 (static IPs only on infra services)
    networks:
      cell-net: {}

    # Capture output to Docker logs (read by log shipper)
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "5"

    healthcheck:
      test: ["CMD-SHELL", "kill -0 1"]
      interval: 30s
      timeout: 5s
      retries: 3

    depends_on:
      - http-proxy
      - dns-filter

    restart: unless-stopped

  # ===========================================================================
  # Cell (STANDARD - gVisor isolation, recommended for production)
  # Requires gVisor: https://gvisor.dev/docs/user_guide/install/
  # Use --profile dev if gVisor is not installed
  # ===========================================================================
  cell:
    build:
      context: .
      dockerfile: cell.Dockerfile
      args:
        - VARIANT=${CELL_VARIANT:-lean}
    image: cell:${CELL_VARIANT:-lean}
    # No container_name — allows `docker compose --scale cell=N`
    labels:
      - "cagent.role=cell"
      - "cagent.log-collect=true"
    profiles:
      - standard

    # gVisor runtime - syscalls intercepted in user-space
    runtime: runsc

    # Strict security options
    security_opt:
      - no-new-privileges:true
      - seccomp:./configs/seccomp/cell-profile.json  # defense-in-depth: host seccomp + gVisor

    # Resource limits (configurable via cagent.yaml → warden → .env)
    deploy:
      resources:
        limits:
          cpus: '${CELL_CPU_LIMIT:-1}'
          memory: ${CELL_MEMORY_LIMIT:-2G}
          pids: ${CELL_PIDS_LIMIT:-256}
        reservations:
          cpus: '0.25'
          memory: 256M

    # DNS forced through dns-filter (dns-filter-2 failover via --profile dns-ha)
    dns:
      - 10.200.1.5
      - 10.200.1.6

    # Environment
    environment:
      - HTTP_PROXY=http://10.200.1.10:8443
      - HTTPS_PROXY=http://10.200.1.10:8443
      - http_proxy=http://10.200.1.10:8443
      - https_proxy=http://10.200.1.10:8443
      - NO_PROXY=localhost,127.0.0.1
      - no_proxy=localhost,127.0.0.1
      - USER_NAME=cell
      - SSH_AUTHORIZED_KEYS=${SSH_AUTHORIZED_KEYS:-}
      - TMUX_AUTO_ATTACH=${TMUX_AUTO_ATTACH:-1}
      - TMUX_SESSION=${TMUX_SESSION:-main}

    # SSH access (set SSH_AUTHORIZED_KEYS to enable)
    ports:
      - "${SSH_PORT:-2222}:22"

    # Volumes (more restricted in secure mode)
    volumes:
      - cell-logs:/var/log/cell
      - cell-workspace:/workspace:rw

    # ONLY on cell-net
    # IP auto-assigned from 10.200.1.0/24 (static IPs only on infra services)
    networks:
      cell-net: {}

    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

    healthcheck:
      test: ["CMD-SHELL", "kill -0 1"]
      interval: 30s
      timeout: 5s
      retries: 3

    depends_on:
      - http-proxy
      - dns-filter

    restart: unless-stopped

  # ===========================================================================
  # DNS Filter (redundant) - Enable with --profile dns-ha
  # Uses the same Corefile as the primary. If dns-filter goes down,
  # cells fall back to this instance automatically (standard DNS client behavior).
  # ===========================================================================
  dns-filter-2:
    profiles: ["dns-ha"]
    image: coredns/coredns:latest
    container_name: dns-filter-2
    labels:
      - "cagent.log-collect=true"
    volumes:
      - ./configs/coredns:/etc/coredns
    command: ["-conf", "/etc/coredns/Corefile"]
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      cell-net:
        ipv4_address: 10.200.1.6
      infra-net:
        ipv4_address: 10.200.2.6
    restart: unless-stopped

  # ===========================================================================
  # DNS Filter - CoreDNS with domain allowlist
  # Bridged to both networks so cell can resolve DNS
  # Corefile generated by warden from CP allowlist + static config
  # ===========================================================================
  dns-filter:
    image: coredns/coredns:latest
    container_name: dns-filter
    labels:
      - "cagent.log-collect=true"
    # Scaling: to run >1 replica, remove container_name and static IPs, add a load balancer
    # deploy:
    #   replicas: ${DNS_FILTER_REPLICAS:-1}

    volumes:
      # Corefile from host (static default, overwritten by warden when it syncs)
      - ./configs/coredns:/etc/coredns

    command: ["-conf", "/etc/coredns/Corefile"]

    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3

    # On BOTH networks - cell-net for cell access, infra-net for upstream
    networks:
      cell-net:
        ipv4_address: 10.200.1.5
      infra-net:
        ipv4_address: 10.200.2.5

    restart: unless-stopped

  # ===========================================================================
  # HTTP Proxy - Egress gateway with logging and credential injection
  # Config generated by warden from cagent.yaml
  # ===========================================================================
  http-proxy:
    image: envoyproxy/envoy:v1.28-latest
    container_name: http-proxy
    labels:
      - "cagent.log-collect=true"
    # Scaling: to run >1 replica, remove container_name and static IPs, add a load balancer
    # deploy:
    #   replicas: ${HTTP_PROXY_REPLICAS:-1}

    volumes:
      # Generated config from warden (falls back to static if not present)
      - proxy-config:/etc/envoy
      # Static config as fallback (copied by entrypoint if no generated config)
      - ./configs/envoy/envoy-enhanced.yaml:/etc/envoy/envoy.default.yaml:ro
      # Entrypoint script
      - ./configs/envoy/entrypoint.sh:/entrypoint.sh:ro

    entrypoint: ["/bin/sh", "/entrypoint.sh"]

    # No env vars needed: ext_authz filter talks to warden (local, no auth)

    # JSON logs to stdout (captured by Docker, read by log shipper)
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

    # On BOTH networks - cell-net for cell access, infra-net for egress + CP access
    networks:
      cell-net:
        ipv4_address: 10.200.1.10
      infra-net:
        ipv4_address: 10.200.2.10

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 128M

    restart: unless-stopped

    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/9901'"]
      interval: 10s
      timeout: 5s
      retries: 3

  # ===========================================================================
  # Log Shipper - Log Collection & Forwarding
  # Collects Docker logs, gVisor audit logs, cell app logs
  # Optional: Only runs with --profile auditing
  # ===========================================================================
  log-shipper:
    image: timberio/vector:0.36.0-alpine
    container_name: log-shipper
    profiles:
      - auditing

    volumes:
      - ./configs/vector/vector.yaml:/etc/vector/vector.yaml:ro
      - ./configs/vector/sinks/${DATAPLANE_MODE:-standalone}.yaml:/etc/vector/sinks.yaml:ro
      - cell-logs:/var/log/cell:ro
      - log-shipper-data:/var/lib/vector
      - log-shipper-backup:/var/log/vector
      # Docker socket for container log collection
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # gVisor logs (requires runsc --debug-log=/var/log/runsc/)
      - /var/log/runsc:/var/log/runsc:ro

    environment:
      # Control plane API (used by connected sinks config)
      - CONTROL_PLANE_URL=${CONTROL_PLANE_URL:-}
      - CONTROL_PLANE_TOKEN=${CONTROL_PLANE_TOKEN:-}
      - ENVIRONMENT=${ENVIRONMENT:-development}

    command: ["--config", "/etc/vector/vector.yaml", "--config", "/etc/vector/sinks.yaml"]

    # Only on infra-net - can reach control plane
    networks:
      - infra-net

    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 64M

    restart: unless-stopped

    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8686/health || kill -0 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # Warden - Unified data plane service
  # Handles config generation, container management, CP sync, and admin UI
  # In admin profile: includes local admin UI (frontend + full API)
  # In managed profile: API-only (no frontend)
  # ===========================================================================
  warden:
    build:
      context: .
      dockerfile: services/warden/Dockerfile
    container_name: warden
    labels:
      - "cagent.log-collect=true"
    profiles:
      - managed
      - admin

    environment:
      # Operation mode: "standalone" or "connected"
      - DATAPLANE_MODE=${DATAPLANE_MODE:-standalone}
      # Control plane connection (for connected mode)
      - CONTROL_PLANE_URL=${CONTROL_PLANE_URL:-}
      - CONTROL_PLANE_TOKEN=${CONTROL_PLANE_TOKEN:-}
      - HEARTBEAT_INTERVAL=${HEARTBEAT_INTERVAL:-30}
      - CONFIG_SYNC_INTERVAL=${CONFIG_SYNC_INTERVAL:-300}
      # Local admin settings
      - DATA_PLANE_DIR=/app/cagent

    volumes:
      # Docker socket for container management
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # Main config file (read-write for admin UI config editing)
      - ./configs/cagent.yaml:/etc/cagent/cagent.yaml
      # CoreDNS config directory (warden overwrites Corefile here)
      - ./configs/coredns:/etc/coredns
      # HTTP proxy config volume (warden writes generated config here)
      - proxy-config:/etc/envoy
      # Seccomp profiles for container recreation
      - ./configs/seccomp/profiles:/etc/seccomp/profiles:ro
      # Data plane directory (for .env and docker-compose access)
      - .:/app/cagent

    ports:
      - "${LOCAL_ADMIN_PORT:-8081}:8080"

    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3

    networks:
      - infra-net

    restart: unless-stopped

  # ===========================================================================
  # Email Proxy - Controlled email access (IMAP/SMTP) for agents
  # Supports Gmail (OAuth2), Outlook (OAuth2), and generic IMAP/SMTP
  # Optional: Only runs with --profile email
  # ===========================================================================
  email-proxy:
    build:
      context: ./services/email_proxy
    container_name: email-proxy
    labels:
      - "cagent.log-collect=true"
    profiles:
      - email

    environment:
      - CAGENT_CONFIG_PATH=/etc/cagent/cagent.yaml
      # Gmail OAuth (set in .env or shell)
      - GMAIL_CLIENT_ID=${GMAIL_CLIENT_ID:-}
      - GMAIL_CLIENT_SECRET=${GMAIL_CLIENT_SECRET:-}
      - GMAIL_REFRESH_TOKEN=${GMAIL_REFRESH_TOKEN:-}
      # Outlook/M365 OAuth (set in .env or shell)
      - OUTLOOK_CLIENT_ID=${OUTLOOK_CLIENT_ID:-}
      - OUTLOOK_CLIENT_SECRET=${OUTLOOK_CLIENT_SECRET:-}
      - OUTLOOK_REFRESH_TOKEN=${OUTLOOK_REFRESH_TOKEN:-}
      # Generic mail password (set in .env or shell)
      - MAIL_PASSWORD=${MAIL_PASSWORD:-}

    volumes:
      - ./configs/cagent.yaml:/etc/cagent/cagent.yaml:ro

    # On infra-net only - cell accesses via Envoy on cell-net
    networks:
      infra-net:
        ipv4_address: 10.200.2.40

    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8025/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

    restart: unless-stopped

# =============================================================================
# Networks
# =============================================================================
networks:
  # Cell network - INTERNAL, no external access
  cell-net:
    name: cagent-cell-net
    driver: bridge
    internal: true  # No external connectivity
    enable_ipv6: false  # Prevent IPv6 bypass of egress controls
    ipam:
      config:
        - subnet: 10.200.1.0/24

  # Infrastructure network - can reach control plane
  infra-net:
    name: cagent-infra-net
    driver: bridge
    enable_ipv6: false  # Prevent IPv6 bypass of egress controls
    ipam:
      config:
        - subnet: 10.200.2.0/24

# =============================================================================
# Volumes
# =============================================================================
volumes:
  cell-logs:
  cell-workspace:
  proxy-config:
  log-shipper-data:
  log-shipper-backup:
